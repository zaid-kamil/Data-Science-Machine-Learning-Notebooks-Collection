{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Optimizing NN models.ipynb","provenance":[{"file_id":"1x6Kx7CYG-gsr29p6-MA23D9zhCaDz_XT","timestamp":1587625210947}],"collapsed_sections":[],"mount_file_id":"1x6Kx7CYG-gsr29p6-MA23D9zhCaDz_XT","authorship_tag":"ABX9TyODZGGrA4gLupfJNgnv6QOs"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"9NMCNPZ0qLlE","colab_type":"text"},"source":["# finding the right architecture and hyperparameters."]},{"cell_type":"markdown","metadata":{"id":"40Ob4Y_7rvsH","colab_type":"text"},"source":["an important step in the machine learning workflow is to identify the best hyperparameters for your problem, which often involves experimentation. This process is known as \"Hyperparameter Optimization\" or \"Hyperparameter Tuning\"."]},{"cell_type":"markdown","metadata":{"id":"aV-p4YSTqVIR","colab_type":"text"},"source":["Here are some of our options\n","\n","\n","1. Hand Tuning or Manual search \n","2. Grid Search\n","3. Random Search\n","4. Bayesian Optimization/Other probabilistic optimizations\n","5. Tree-structured Parzen Estimator Approach\n","\n","---\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"SJZyLs_xn6iG","colab_type":"code","colab":{}},"source":["%tensorflow_version 2.x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KZFIydEKoAa1","colab_type":"code","colab":{}},"source":["import tensorflow as tf"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zO_C5wp3naT-","colab_type":"text"},"source":["# grid search example"]},{"cell_type":"code","metadata":{"id":"v-ZzAbpanuqL","colab_type":"code","colab":{}},"source":["# Load the TensorBoard notebook extension\n","%load_ext tensorboard\n","\n","# Clear any logs from previous runs\n","!rm -rf ./logs/ "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6XDBC35bxz7f","colab_type":"code","colab":{}},"source":["from tensorboard.plugins.hparams import api as hp"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GOn0M64Lx2qM","colab_type":"code","colab":{}},"source":["fashion_mnist = tf.keras.datasets.fashion_mnist\n","\n","(x_train, y_train),(x_test, y_test) = fashion_mnist.load_data()\n","x_train, x_test = x_train / 255.0, x_test / 255.0"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VjSCWPM8x7ES","colab_type":"text"},"source":["# Experiment setup and the HParams experiment summary\n","Experiment with three hyperparameters in the model:\n","\n","- Number of units in the first dense layer\n","- Dropout rate in the dropout layer\n","- Optimizer\n","\n","List the values to try, and log an experiment configuration to TensorBoard. This step is optional: you can provide domain information to enable more precise filtering of hyperparameters in the UI, and you can specify which metrics should be displayed.\n"]},{"cell_type":"code","metadata":{"id":"cjO_nOnex4pX","colab_type":"code","colab":{}},"source":["HP_NUM_UNITS = hp.HParam('num_units', hp.Discrete([16, 32]))\n","HP_DROPOUT = hp.HParam('dropout', hp.RealInterval(0.1, 0.2))\n","HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'sgd']))\n","METRIC_ACCURACY = 'accuracy'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aWfCGRNJyyGK","colab_type":"code","colab":{}},"source":["with tf.summary.create_file_writer('logs/hparam_tuning').as_default():\n","    hp.hparams_config( hparams=[HP_NUM_UNITS, HP_DROPOUT, HP_OPTIMIZER], metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')],)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YufXPc3SzCSg","colab_type":"text"},"source":["# Adapt TensorFlow runs to log hyperparameters and metrics\n","The model will be quite simple: two dense layers with a dropout layer between them. The training code will look familiar, although the hyperparameters are no longer hardcoded. Instead, the hyperparameters are provided in an hparams dictionary and used throughout the training function"]},{"cell_type":"code","metadata":{"id":"ehOvxqtDy3LJ","colab_type":"code","colab":{}},"source":["def train_test_model(hparams):\n","    model = tf.keras.models.Sequential([\n","        tf.keras.layers.Flatten(),\n","        tf.keras.layers.Dense(hparams[HP_NUM_UNITS], activation=tf.nn.relu),\n","        tf.keras.layers.Dropout(hparams[HP_DROPOUT]),\n","        tf.keras.layers.Dense(10, activation=tf.nn.softmax),\n","    ])\n","    model.compile(\n","        optimizer=hparams[HP_OPTIMIZER],\n","        loss='sparse_categorical_crossentropy',\n","        metrics=['accuracy'],\n","    )\n","\n","    model.fit(x_train, y_train, epochs=1) # Run with 1 epoch to speed things up for demo purposes\n","    _, accuracy = model.evaluate(x_test, y_test)\n","    return accuracy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4rtJ3Gnfzz2I","colab_type":"code","colab":{}},"source":["def run(run_dir, hparams):\n","  with tf.summary.create_file_writer(run_dir).as_default():\n","    hp.hparams(hparams)  # record the values used in this trial\n","    accuracy = train_test_model(hparams)\n","    tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YMQtnIEbz97A","colab_type":"text"},"source":["# Start runs and log them all under one parent directory\n","For simplicity, use a grid search: try all combinations of the discrete parameters and just the lower and upper bounds of the real-valued parameter. \n","\n","For more complex scenarios, it might be more effective to choose each hyperparameter value randomly (this is called a random search)."]},{"cell_type":"code","metadata":{"id":"5ZSySYrcz0ax","colab_type":"code","colab":{}},"source":["session_num = 0\n","\n","for num_units in HP_NUM_UNITS.domain.values:\n","  for dropout_rate in (HP_DROPOUT.domain.min_value, HP_DROPOUT.domain.max_value):\n","    for optimizer in HP_OPTIMIZER.domain.values:\n","      hparams = {\n","          HP_NUM_UNITS: num_units,\n","          HP_DROPOUT: dropout_rate,\n","          HP_OPTIMIZER: optimizer,\n","      }\n","      run_name = \"run-%d\" % session_num\n","      print('--- Starting trial: %s' % run_name)\n","      print({h.name: hparams[h] for h in hparams})\n","      run('logs/hparam_tuning/' + run_name, hparams)\n","      session_num += 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IKVo_f-E0UoP","colab_type":"code","colab":{}},"source":["%tensorboard --logdir logs/hparam_tuning"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I1d_JJhL0du0","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}